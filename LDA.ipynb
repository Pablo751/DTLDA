import os
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# Setup logging
import logging

# NLTK downloads (can be excluded once downloaded)
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Text Processing
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist
from nltk import pos_tag
from textblob import TextBlob

# Word Cloud
from wordcloud import WordCloud

# Topic Modeling
import gensim
from gensim.corpora import Dictionary
from gensim.models import LdaModel, Phrases, TfidfModel, CoherenceModel
import pyLDAvis.gensim_models


# Define the directory where the corpus is located
source_dir = '/Users/juanpablocasado/Downloads/OneDrive_1_7-8-2023/txt'  #Source directory of all the original documents in txt format
output_dir = '/Users/juanpablocasado/Downloads/OneDrive_1_7-8-2023/cleanedtxt'  #Output directory of all the cleaned documents in txt format


# Define the cleaning function without lemmatization
def clean_text_without_lemmatization(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Only keeping alphabets and spaces
    words = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    return words

# Define the cleaning function with lemmatization
def get_wordnet_pos(treebank_tag):
    """Map treebank POS tag to first character used by WordNetLemmatizer"""
    tag = {
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV,
        'J': wordnet.ADJ
    }.get(treebank_tag[0], wordnet.NOUN)
    return tag

def clean_text_with_lemmatization(text):
    words = clean_text_without_lemmatization(text)  # Using the tokenization from the newer script
    lemmatizer = WordNetLemmatizer()
    tagged_words = pos_tag(words)
    # Only keep nouns (starting with 'N') and adjectives (starting with 'J')
    words = [lemmatizer.lemmatize(word) for word, pos in tagged_words if pos.startswith('N') or pos.startswith('J')]
    return words

# Load and clean the corpus based on the cleaning function provided
def load_and_clean_corpus_as_documents(directory_path, cleaning_function):
    all_documents = []
    for file_name in os.listdir(directory_path):
        with open(os.path.join(directory_path, file_name), 'r', encoding='utf-8', errors='ignore') as file:
            content = file.read()
            document_words = cleaning_function(content)
            all_documents.append(document_words)
    return all_documents

# Display the word frequency distribution graph
def plot_word_frequency(words, top_n=30, title_suffix=""):
    freq_dist = FreqDist(words)
    plt.figure(figsize=(15,6))
    freq_dist.plot(top_n, title=f"Top {top_n} Most Common Words {title_suffix}")
    plt.show()

# Load and clean the corpus without lemmatization
all_words_without_lemmatization = load_and_clean_corpus(source_dir, clean_text_without_lemmatization)

# Plot the word frequency distribution for cleaned but not lemmatized text
plot_word_frequency(all_words_without_lemmatization, title_suffix="After Cleaning (No Lemmatization)")

# Load and clean the corpus with lemmatization
all_words_with_lemmatization = load_and_clean_corpus(source_dir, clean_text_with_lemmatization)

# Plot the word frequency distribution for cleaned and lemmatized text
plot_word_frequency(all_words_with_lemmatization, title_suffix="After Cleaning and Lemmatization")

# Generate and display a word cloud
def generate_word_cloud(words, title):
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(" ".join(words))
    plt.figure(figsize=(15,8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Generate word cloud for cleaned but not lemmatized text
generate_word_cloud(all_words_without_lemmatization, title="Word Cloud After Cleaning (No Lemmatization)")

# Generate word cloud for cleaned and lemmatized text
generate_word_cloud(all_words_with_lemmatization, title="Word Cloud After Cleaning and Lemmatization")
